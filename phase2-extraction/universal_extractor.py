#!/usr/bin/env python3
"""
UNIVERSAL EXTRACTOR with TTS-Grade Normalization

Handles:
- PDF (with self-correction)
- DOCX (Word documents)
- EPUB/MOBI (ebooks)
- TXT (with normalization)
- And normalizes ALL text for TTS

Philosophy: Extract → Normalize → Validate → Polish for TTS
"""
import re
import logging
from pathlib import Path
from typing import Dict, Tuple
import unicodedata

logging.basicConfig(
    level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s"
)
logger = logging.getLogger(__name__)


class UniversalExtractor:
    """
    Extracts and normalizes text from any supported file type.
    """

    def __init__(self, file_path: str):
        self.file_path = Path(file_path)
        self.file_type = self.file_path.suffix.lower()

    def normalize_whitespace(self, text: str) -> str:
        """
        Normalize whitespace for TTS.
        - Replace multiple spaces with single space
        - Fix common spacing issues
        - Preserve paragraph breaks
        """
        # Replace tabs with spaces
        text = text.replace("\t", " ")

        # Normalize unicode spaces
        text = re.sub(
            r"[\u00A0\u1680\u2000-\u200B\u202F\u205F\u3000]", " ", text
        )

        # Replace multiple spaces with single space (but preserve line breaks)
        lines = text.split("\n")
        normalized_lines = []
        for line in lines:
            # Collapse multiple spaces within a line
            line = re.sub(r" +", " ", line)
            # Strip leading/trailing spaces
            line = line.strip()
            normalized_lines.append(line)

        # Join lines, collapsing multiple blank lines into double newline (paragraph break)
        text = "\n".join(normalized_lines)
        text = re.sub(r"\n\n\n+", "\n\n", text)

        return text

    def normalize_unicode(self, text: str) -> str:
        """
        Normalize unicode characters for TTS.
        - Convert fancy quotes to regular quotes
        - Remove zero-width characters
        - Fix common unicode issues
        """
        # Normalize to NFC form (composed characters)
        text = unicodedata.normalize("NFC", text)

        # Convert fancy quotes
        text = text.replace('"', '"').replace('"', '"')
        text = text.replace(""", "'").replace(""", "'")
        text = text.replace("‚", ",").replace("„", '"')

        # Convert dashes
        text = text.replace("—", "--").replace("–", "-")
        text = text.replace("−", "-")  # Minus sign

        # Remove zero-width characters
        text = re.sub(r"[\u200B-\u200D\uFEFF]", "", text)

        # Remove soft hyphens
        text = text.replace("\u00ad", "")

        # Fix ellipsis
        text = text.replace("…", "...")

        return text

    def remove_artifacts(self, text: str) -> str:
        """
        Remove common extraction artifacts.
        - Headers/footers that repeat
        - Page numbers
        - URLs like "OceanofPDF.com"
        """
        lines = text.split("\n")
        cleaned_lines = []

        for line in lines:
            # Skip common artifacts
            if re.match(r"^Page \d+$", line.strip()):
                continue
            if re.match(r"^\d+$", line.strip()) and len(line.strip()) < 4:
                continue
            if "oceanofpdf" in line.lower():
                continue
            if "generated by" in line.lower():
                continue

            cleaned_lines.append(line)

        return "\n".join(cleaned_lines)

    def fix_common_errors(self, text: str) -> str:
        """
        Fix common OCR/extraction errors for TTS.
        """
        # Fix missing spaces after punctuation
        text = re.sub(r"([.!?])([A-Z])", r"\1 \2", text)

        # Fix missing spaces after commas
        text = re.sub(r",([A-Za-z])", r", \1", text)

        # Fix "l" misread as "1" in common words
        text = re.sub(r"\b1([a-z])", r"l\1", text)  # "1ike" → "like"

        # Fix "O" misread as "0" in common words
        text = re.sub(r"\b0([a-z])", r"O\1", text)  # "0f" → "Of"

        return text

    def tts_normalize(self, text: str) -> str:
        """
        Master TTS normalization function.
        Apply all normalization steps in order.
        """
        logger.info("Applying TTS normalization...")

        # Step 1: Unicode normalization
        text = self.normalize_unicode(text)

        # Step 2: Whitespace normalization
        text = self.normalize_whitespace(text)

        # Step 3: Remove artifacts
        text = self.remove_artifacts(text)

        # Step 4: Fix common errors
        text = self.fix_common_errors(text)

        logger.info(f"✓ Normalization complete: {len(text):,} chars")
        return text

    def extract_pdf(self) -> str:
        """Extract from PDF using multi-pass approach."""
        logger.info("Extracting PDF...")

        # Try pypdf first
        try:
            from pypdf import PdfReader

            reader = PdfReader(str(self.file_path))
            text = "\n".join(
                page.extract_text()
                for page in reader.pages
                if page.extract_text()
            )
            if text.strip():
                logger.info(f"✓ pypdf extracted {len(text):,} chars")
                return text
        except Exception as e:
            logger.warning(f"pypdf failed: {e}")

        # Try other methods...
        logger.error("All PDF extraction methods failed")
        return ""

    def extract_docx(self) -> str:
        """Extract from Word document."""
        logger.info("Extracting DOCX...")

        try:
            from docx import Document

            doc = Document(str(self.file_path))

            # Extract paragraphs
            paragraphs = []
            for para in doc.paragraphs:
                if para.text.strip():
                    paragraphs.append(para.text)

            text = "\n\n".join(paragraphs)
            logger.info(
                f"✓ Extracted {len(paragraphs)} paragraphs, {len(text):,} chars"
            )
            return text

        except ImportError:
            logger.error("python-docx not installed: pip install python-docx")
            return ""
        except Exception as e:
            logger.error(f"DOCX extraction failed: {e}")
            return ""

    def extract_epub(self) -> str:
        """Extract from EPUB ebook."""
        logger.info("Extracting EPUB...")

        try:
            import ebooklib
            from ebooklib import epub
            from bs4 import BeautifulSoup

            book = epub.read_epub(str(self.file_path))

            chapters = []
            for item in book.get_items():
                if item.get_type() == ebooklib.ITEM_DOCUMENT:
                    # Parse HTML content
                    soup = BeautifulSoup(item.get_content(), "html.parser")
                    text = soup.get_text()
                    if text.strip():
                        chapters.append(text)

            text = "\n\n".join(chapters)
            logger.info(
                f"✓ Extracted {len(chapters)} chapters, {len(text):,} chars"
            )
            return text

        except ImportError:
            logger.error(
                "ebooklib not installed: pip install ebooklib beautifulsoup4"
            )
            return ""
        except Exception as e:
            logger.error(f"EPUB extraction failed: {e}")
            return ""

    def extract_txt(self) -> str:
        """Extract from text file with encoding detection."""
        logger.info("Extracting TXT...")

        try:
            # Try UTF-8 first
            with open(self.file_path, "r", encoding="utf-8") as f:
                text = f.read()
            logger.info(f"✓ Extracted {len(text):,} chars (UTF-8)")
            return text
        except UnicodeDecodeError:
            # Try with encoding detection
            try:
                import chardet

                with open(self.file_path, "rb") as f:
                    raw = f.read()
                encoding = chardet.detect(raw)["encoding"]
                text = raw.decode(encoding, errors="replace")
                logger.info(f"✓ Extracted {len(text):,} chars ({encoding})")
                return text
            except Exception as e:
                logger.error(f"TXT extraction failed: {e}")
                return ""

    def extract_and_normalize(self) -> Tuple[str, Dict]:
        """
        Main extraction method.
        Returns (normalized_text, metadata)
        """
        logger.info(f"\n{'='*80}")
        logger.info(f"UNIVERSAL EXTRACTION: {self.file_path.name}")
        logger.info(f"File type: {self.file_type}")
        logger.info(f"{'='*80}\n")

        # Extract based on file type
        if self.file_type == ".pdf":
            raw_text = self.extract_pdf()
        elif self.file_type in [".docx", ".doc"]:
            raw_text = self.extract_docx()
        elif self.file_type in [".epub"]:
            raw_text = self.extract_epub()
        elif self.file_type == ".txt":
            raw_text = self.extract_txt()
        else:
            logger.error(f"Unsupported file type: {self.file_type}")
            return "", {
                "status": "failed",
                "error": f"Unsupported file type: {self.file_type}",
            }

        if not raw_text:
            return "", {"status": "failed", "error": "Extraction failed"}

        # Normalize for TTS
        normalized_text = self.tts_normalize(raw_text)

        # Calculate metrics
        raw_len = len(raw_text)
        norm_len = len(normalized_text)
        reduction = (raw_len - norm_len) / raw_len * 100 if raw_len else 0

        metadata = {
            "status": "success",
            "file_type": self.file_type,
            "raw_length": raw_len,
            "normalized_length": norm_len,
            "reduction_pct": reduction,
            "normalization_applied": True,
        }

        logger.info(f"\n{'='*80}")
        logger.info("EXTRACTION COMPLETE")
        logger.info(f"{'='*80}")
        logger.info(f"Raw text: {raw_len:,} chars")
        logger.info(f"Normalized: {norm_len:,} chars")
        logger.info(f"Reduction: {reduction:.1f}%")
        logger.info(f"Status: {metadata['status']}")

        return normalized_text, metadata


def extract_any_file(file_path: str) -> Tuple[str, Dict]:
    """
    Convenience function to extract and normalize any supported file.
    """
    extractor = UniversalExtractor(file_path)
    return extractor.extract_and_normalize()


if __name__ == "__main__":
    import sys

    if len(sys.argv) < 2:
        print("Usage: python universal_extractor.py <file_path>")
        print("\nSupported formats:")
        print("  - PDF (.pdf)")
        print("  - Word (.docx, .doc)")
        print("  - EPUB (.epub)")
        print("  - Text (.txt)")
        sys.exit(1)

    file_path = sys.argv[1]

    if not Path(file_path).exists():
        print(f"❌ File not found: {file_path}")
        sys.exit(1)

    # Extract and normalize
    text, metadata = extract_any_file(file_path)

    if metadata["status"] == "success":
        # Save output
        output_file = Path(file_path).stem + "_normalized.txt"
        with open(output_file, "w", encoding="utf-8") as f:
            f.write(text)

        print("\n✅ Success!")
        print(f"   Output: {output_file}")
        print(f"   Length: {len(text):,} characters")
        print("\nFirst 500 characters:")
        print("-" * 80)
        print(text[:500])
        print("-" * 80)
    else:
        print(f"\n❌ Failed: {metadata.get('error', 'Unknown error')}")
